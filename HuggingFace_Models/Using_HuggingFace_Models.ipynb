{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b4533b-ef64-49bb-a918-992781bd6cd4",
   "metadata": {},
   "source": [
    "## Using Hugging Face Transformers\n",
    "This notebook demonstartes the usage of a variety of Hugging Face transformers (LLM models) by the pipeline includeing\n",
    "* Sentimental Analysis by a distilbert-base model\n",
    "* How to fine tune a distilbert-base model using labelled data for text classifications\n",
    "* text generation using gpt-2 model\n",
    "* question and answer using a distilbert-base model\n",
    "* translation from English to Chinese use Helsinki-NLP/opus-mt-en-zh model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39605e05-53c6-46e9-927b-45a6ff1bfabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 sentencepiece\\\n",
    "    torchdata==0.5.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25517ee5-6767-45db-9dc3-fd0b2d571fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import evaluate, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7b774-6c74-4ee6-8fbd-f9afd8765586",
   "metadata": {},
   "source": [
    "### Sentimental Analysis\r\n",
    "* load model and tokenizer for a specific model. (here we use a distilbert-base mode, which is an encoder transformerl)\r\n",
    "* pass the model and tokenizer to transformer pipeline, specify the purpose as \"sentiment-analysis\"\r\n",
    "* pass the sentences we want to analyze, and get labels and the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3737273e-c292-4d53-a20e-e0a220ecf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, GenerationConfig\n",
    "\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "\n",
    "# We call define a model object\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b70c23-cc89-44d5-8f43-dfcfb4c6104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call the tokenizer class\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab7441e-a1fa-4826-97ee-47f69c46e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "# Initializing a classifier with a model and a tokenizer\n",
    "classifier = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c62e643-c0e7-4cd4-9b37-9d00e5230b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.8878770470619202}, {'label': 'positive', 'score': 0.9786158800125122}]\n"
     ]
    }
   ],
   "source": [
    "labels = classifier([\"I am OK with this!\", \"I love this product.\"])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab701c0-fa37-46fe-ba7b-d8d3d99ae0cf",
   "metadata": {},
   "source": [
    "### Fine Tune the Model for text classifications\n",
    "* load the twitter-sentiment-analysis dataset for fine tuning\n",
    "* the dataset has text and the corresponding labels of postive/negative\n",
    "* First, construct the dataset for training by defining a `tokenize_function` and process the dataset using dataset's map function in batch mode\n",
    "  + Notice the difference between different models to define `tokenize_function`\n",
    "  + In FLAN-T5, tokenize_function can generate both input_id and label columns for each example and return the example\n",
    "  + For distilled bert model here, only process input text and return the tokenized_ids as the compatible dictionary\n",
    "* set up the TrainingArgument and Trainer to train the model. To reduce time, we only run 1 epoch with `max_step` = 1\n",
    "* a full training will require GPU and maybe a much longer time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c3863-8851-4cfd-b42b-2a6b7305434e",
   "metadata": {},
   "source": [
    "##### Prepare dataset for training\n",
    "* load dataset, check that the dataset contains 'feeling' and 'text' column\n",
    "* rename 'feeling' column to 'labels' for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c938a36c-be24-43ea-aa78-8154ecdca62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\", trust_remote_code=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df203ebb-39ed-42f2-b46a-c7684f2cdaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fa6ami86 so happy that salman won.  btw the 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@phantompoptart .......oops.... I guess I'm ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@bradleyjp decidedly undecided. Depends on the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mountgrace lol i know! its so frustrating isn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kathystover Didn't go much of any where - Lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  feeling\n",
       "0  @fa6ami86 so happy that salman won.  btw the 1...        0\n",
       "1  @phantompoptart .......oops.... I guess I'm ki...        0\n",
       "2  @bradleyjp decidedly undecided. Depends on the...        1\n",
       "3  @Mountgrace lol i know! its so frustrating isn...        1\n",
       "4  @kathystover Didn't go much of any where - Lif...        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the structure of the dataset\n",
    "train_pd = dataset[\"train\"].to_pandas()\n",
    "train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0457a3-207e-414b-88b9-de5b1d73b47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the target variable column only has two unique values. This is a binary classification.\n",
    "train_pd['feeling'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c95be51-2956-4a9f-bf75-67d902a7f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the target column to 'label'\n",
    "dataset = dataset.rename_column('feeling', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6aa95b-8308-42a9-a481-7798e8d33a65",
   "metadata": {},
   "source": [
    "##### Define `tokeniz_function` to process the input text by tokenizer\r\n",
    "* load tokenizer\r\n",
    "* define `toknenize_function`\r\n",
    "* apply `tokenize_function` to dataset using its map() function in batch mode\r\n",
    "* shuffle and only select 5 examples from dataset to demonstrate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c96e21f-b9b4-4dc8-9094-57892963a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126ea0eb71d4a7f9cfab3709e25e3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in FLAN-T5, tokenize_function can generate both input_id and label columns for each example\n",
    "# and return the example. However, for distilled bert model, you can only process\n",
    "# input text and return the tokenized_ids as the compatible dictionary. you can not\n",
    "# return the entire example as the dirctionary\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa5821b-99c7-4848-b7bc-79035cb8f4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 119988\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158800aa-985d-418b-8be0-9424377b0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb371133-2421-4e0d-9ed6-72aadf03d033",
   "metadata": {},
   "source": [
    "##### Set Up TrainingArguments and Trainer\n",
    "* Define `compute_metrics` and start training\n",
    "* Notice that the `compute_metrics` defines the accuracy to be used in this training\n",
    "* I only run for one step and one epoch to save training time and demostrate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f65d1d-9e50-46e7-80a4-f561c47419ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huang\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.573100</td>\n",
       "      <td>1.195367</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1953672170639038,\n",
       " 'eval_accuracy': 0.4,\n",
       " 'eval_runtime': 3.3283,\n",
       " 'eval_samples_per_second': 1.502,\n",
       " 'eval_steps_per_second': 0.3,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer_output\", \n",
    "    logging_dir='./logs', \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "\n",
    "# define the metric to be used in compute_metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d45341-23b7-48a6-b551-3f5d437e62b9",
   "metadata": {},
   "source": [
    "### Text Generation\r\n",
    "* generate text using gpt2 model\r\n",
    "* construct a prompt with the start of a paragraph\r\n",
    "* extract the 'generated_text' en\n",
    "* In this example, the model generated a short paragraph to continue the prompt given as the start of the paragraph\n",
    "* This is a typical use case of decoder transformer, such as GPT2 modeltry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786671f6-2620-4d6d-bc53-a26e76642936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world dominated by AI, it's hard to imagine a more important issue for the future of the human race.\n",
      "\n",
      "\"We're going to have to start thinking about how we can make AI more accessible to people,\" said Dr. David S. Siegel, a professor\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "prompt = \"In a world dominated by AI,\"\n",
    "\n",
    "generation_config = GenerationConfig(max_new_tokens=50, pad_token_id=50256, num_beams=1)\n",
    "generated_text = generator(prompt, generation_config=generation_config)[0]['generated_text']\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76221bd-8f01-4ccf-9655-38d7ccb843d1",
   "metadata": {},
   "source": [
    "### Question-Anwsering\n",
    "* use a distilbert-base model\n",
    "* construct question and context and send to the `qa_pipeline`\n",
    "* retrieve the answer from the context. In this example, the model successfully extracted the population of Paris from the given context\n",
    "* This demonstrate a typical encoder transformer for question-answer, such as the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b21f4c-ad28-4cf3-a691-6ae753e57002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.954908013343811, 'start': 121, 'end': 130, 'answer': '2,140,526'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "context = \"\"\"Paris is the capital and most populous city of France. The city has an area of 105 square kilometers and a population of 2,140,526 residents.\"\"\"\n",
    "question = \"What is the population of Paris?\"\n",
    "\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba07cf7-c6a6-4e6a-a1dc-9ec708a83d91",
   "metadata": {},
   "source": [
    "### Translation\n",
    "* use Helsinkin-NLP/opus-mt-en-zh model to translate English to Chinese\n",
    "* This is a use case for a typical encoder-decode transformer\n",
    "* translate a sentence from the lyrics of my daughter's favorate song \"The conconut song\" by Jeff Lau\n",
    "* the translation is amazing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefea100-674f-478c-bd70-de901841b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Text2TextGenerationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "text2text_generator = Text2TextGenerationPipeline(model, tokenizer)\n",
    "\n",
    "translation = text2text_generator(\"translate English to Chinese: The coconut bark for the kitchen floor If you save some of it, you can build a door\", max_length=512, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74268461-52a8-4b08-b704-df53efbd5339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '英文译为中文:厨房地板的椰子树皮,如果你节省一部分,你可以建造一扇门。'}]\n"
     ]
    }
   ],
   "source": [
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
