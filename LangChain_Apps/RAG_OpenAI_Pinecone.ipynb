{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8fde4c-9222-4265-b72b-8d7693520250",
   "metadata": {},
   "source": [
    "# Achieving RAG for chatbot using OpenAI API and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e302e1c-4c18-4c44-87fd-ba935c3a0853",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "source": [
    "This notebook is to demonstrate the RAG using Langchain's OpenAI API and Piencone vector database. The basic idea is to use pinecone to store and retrieve context information relavant to the request queries and therefore, augment the generated responses. The key processes including the convertion and storage of information/text content to vectors and then store in vector database, and the retrival of text content relavant to the query using vector database, and then use the retrived content as the context to the query.\n",
    "\n",
    "Please make sure you have set the [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io) to use OpenAI and Pinecone in this notebook\n",
    "\n",
    "Packages required for this notebook and the versions used here are the following:\n",
    "* langchain version 0.0.293\n",
    "* openai version 0.28.0\n",
    "* datasets version 2.10.1\n",
    "* pinecone-client version 2.2.4\n",
    "* tiktoken version 0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96e3639-1515-47ce-9cab-21b2c8a43c64",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6862,
    "lastExecutedAt": 1703639528606,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install -qU \\\n    langchain==0.0.292 \\\n    openai==0.28.0 \\\n    datasets==2.10.1 \\\n    pinecone-client==2.2.4 \\\n    tiktoken==0.5.1",
    "outputsMetadata": {
     "0": {
      "height": 277,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.292 \\\n",
    "    openai==0.28.0 \\\n",
    "    datasets==2.10.1 \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9caca-70fd-4ac0-aa15-1bee55c456d3",
   "metadata": {},
   "source": [
    "## Task 1: Building a Chatbot\n",
    "* In langchain, we pass messages to a ChatOpenAI object, with `model_name` specified such as gpt-3.5-tubo etc. \n",
    "* messages we pass consists of messages for different roles. In langchain, there are three types of messages:\n",
    "  + SystemMessage: defines the role the gpt model is playing in the conversation\n",
    "  + HumanMessage: defines the message human sent to the chat. This type of messages are usually our queries\n",
    "  + AIMessage: the message generated by chat as the response to HumanMessages\n",
    "* In the following exmaple, we build messages containing these three messages, and check what a ChatOpenAI's response looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81d6d0e-986b-49f4-94e1-7315a7f0bd67",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1563,
    "lastExecutedAt": 1703639530171,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.chat_models import ChatOpenAI\n\n# initiate a ChatOpenAI object with gpt 3.5 turbo model\nchat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# initiate a ChatOpenAI object with gpt 3.5 turbo model\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f542ae9-c4a0-41aa-a6b6-45585990c246",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10903,
    "lastExecutedAt": 1703639541075,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.schema import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\n# define the messages and their roles to start the conversation\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]\n\n# send the message to the chat\nres = chat(messages)\nres"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! String theory is a theoretical framework in physics that aims to explain the fundamental nature of particles and their interactions by modeling them as tiny vibrating strings. It suggests that all particles in the universe, including the ones that make up matter and the ones that transmit forces, are ultimately composed of these tiny strings.\\n\\nIn string theory, the basic building blocks of the universe are not point-like particles, but rather one-dimensional strings. These strings can vibrate in different ways, giving rise to different fundamental particles. Each vibrational mode corresponds to a different particle with specific properties such as mass and charge.\\n\\nOne of the key ideas in string theory is that it requires extra spatial dimensions beyond the three (length, width, and height) that we are familiar with. These extra dimensions are typically compactified, meaning they are curled up into tiny, tightly wound loops that are too small to be directly observed. The number of dimensions in string theory can vary depending on the specific version of the theory being considered, but the most widely studied version, called superstring theory, requires ten spacetime dimensions (nine spatial dimensions plus time).\\n\\nString theory also incorporates the principles of quantum mechanics, which describes the behavior of particles at the smallest scales. It provides a consistent framework for reconciling quantum mechanics with Einstein's theory of general relativity, which describes gravity as the curvature of spacetime.\\n\\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. It is currently an active area of research in theoretical physics, and scientists are exploring its implications and attempting to find experimental evidence that could support or refute the theory.\\n\\nI hope this gives you a basic understanding of string theory! Let me know if you have any more questions.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "# define the messages and their roles to start the conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]\n",
    "\n",
    "# send the message to the chat\n",
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c51c7-bbbb-4f0f-b218-850221f3dcdf",
   "metadata": {},
   "source": [
    "You can see that the response object has the following properties\n",
    "* it is organized as a dictionary and the content we are interested in is in \"content\" key\n",
    "* However, you can't retrieve the content using `res['content']` as in a dictionary, you have to use dot reference operation\n",
    "* the response itself is an AIMessage object\n",
    "\n",
    "Let's print the 'content' of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fae8e0-c182-4551-8493-e954e3c983ad",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1703639541125,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 357,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! String theory is a theoretical framework in physics that aims to explain the fundamental nature of particles and their interactions by modeling them as tiny vibrating strings. It suggests that all particles in the universe, including the ones that make up matter and the ones that transmit forces, are ultimately composed of these tiny strings.\n",
      "\n",
      "In string theory, the basic building blocks of the universe are not point-like particles, but rather one-dimensional strings. These strings can vibrate in different ways, giving rise to different fundamental particles. Each vibrational mode corresponds to a different particle with specific properties such as mass and charge.\n",
      "\n",
      "One of the key ideas in string theory is that it requires extra spatial dimensions beyond the three (length, width, and height) that we are familiar with. These extra dimensions are typically compactified, meaning they are curled up into tiny, tightly wound loops that are too small to be directly observed. The number of dimensions in string theory can vary depending on the specific version of the theory being considered, but the most widely studied version, called superstring theory, requires ten spacetime dimensions (nine spatial dimensions plus time).\n",
      "\n",
      "String theory also incorporates the principles of quantum mechanics, which describes the behavior of particles at the smallest scales. It provides a consistent framework for reconciling quantum mechanics with Einstein's theory of general relativity, which describes gravity as the curvature of spacetime.\n",
      "\n",
      "However, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. It is currently an active area of research in theoretical physics, and scientists are exploring its implications and attempting to find experimental evidence that could support or refute the theory.\n",
      "\n",
      "I hope this gives you a basic understanding of string theory! Let me know if you have any more questions.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090e9e9-880d-4b29-909a-a9653474c8b1",
   "metadata": {},
   "source": [
    "Since the response is just an `AIMessage` object, we can append it to our original `messages`, append another `HumanMessage`,  and send the resuling message to the chat to continue the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be31574f-4801-452e-94a1-a544e50fb4b1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1703639541172,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages.append(res)\nprompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\nmessages.append(prompt)"
   },
   "outputs": [],
   "source": [
    "messages.append(res)\n",
    "prompt = HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cb41f-1b50-4d2f-9e8e-b461ad2c0b17",
   "metadata": {},
   "source": [
    "As shown below, the resulting messages contains all of our conversation messages from the begining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc0c721-8eb2-4a20-8491-89a281480ea5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1703639541225,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}),\n",
       " HumanMessage(content='Hi AI, how are you today?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"I'd like to understand string theory.\", additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Sure! String theory is a theoretical framework in physics that aims to explain the fundamental nature of particles and their interactions by modeling them as tiny vibrating strings. It suggests that all particles in the universe, including the ones that make up matter and the ones that transmit forces, are ultimately composed of these tiny strings.\\n\\nIn string theory, the basic building blocks of the universe are not point-like particles, but rather one-dimensional strings. These strings can vibrate in different ways, giving rise to different fundamental particles. Each vibrational mode corresponds to a different particle with specific properties such as mass and charge.\\n\\nOne of the key ideas in string theory is that it requires extra spatial dimensions beyond the three (length, width, and height) that we are familiar with. These extra dimensions are typically compactified, meaning they are curled up into tiny, tightly wound loops that are too small to be directly observed. The number of dimensions in string theory can vary depending on the specific version of the theory being considered, but the most widely studied version, called superstring theory, requires ten spacetime dimensions (nine spatial dimensions plus time).\\n\\nString theory also incorporates the principles of quantum mechanics, which describes the behavior of particles at the smallest scales. It provides a consistent framework for reconciling quantum mechanics with Einstein's theory of general relativity, which describes gravity as the curvature of spacetime.\\n\\nHowever, it's important to note that string theory is still a work in progress and has not yet been experimentally confirmed. It is currently an active area of research in theoretical physics, and scientists are exploring its implications and attempting to find experimental evidence that could support or refute the theory.\\n\\nI hope this gives you a basic understanding of string theory! Let me know if you have any more questions.\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683f2fb-d1b5-46bf-877b-6f717360c7bc",
   "metadata": {},
   "source": [
    "Now, let's send the messages to chat and see what response we will get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f68e1d4-4dcc-4dbe-b7ce-df321f90ae5c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14092,
    "lastExecutedAt": 1703639555317,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "res = chat(messages)\n\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 397,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory because it aims to describe all fundamental particles and forces within a single framework. The ultimate goal is to unify all the fundamental forces of nature, including gravity, electromagnetism, the strong nuclear force, and the weak nuclear force, into a single, consistent theory.\n",
      "\n",
      "Currently, there are multiple fundamental forces described by separate theories in physics, each with its own set of rules and equations. For example, gravity is described by Einstein's theory of general relativity, while the other forces are described by the Standard Model of particle physics.\n",
      "\n",
      "String theory offers the possibility of unification by treating all particles as different vibrational modes of tiny strings. This means that the underlying structure of the universe is unified, and the different particles and forces arise from different vibrations of the same fundamental entities.\n",
      "\n",
      "Furthermore, string theory naturally incorporates both quantum mechanics and general relativity, which are the two pillars of modern physics. This is significant because these two theories are currently incompatible with each other when applied to extreme conditions, such as the very early universe or black holes. String theory provides a framework where both quantum mechanics and general relativity can coexist and be consistent with each other.\n",
      "\n",
      "While string theory has not yet been proven experimentally, it has shown promise in resolving some long-standing problems in physics, such as the ultraviolet divergences in quantum field theory and the hierarchy problem (i.e., why the gravitational force is so much weaker than the other forces). It also suggests the existence of additional particles, such as supersymmetric partners, which could help explain certain phenomena.\n",
      "\n",
      "However, it's important to note that there are still many open questions and challenges in string theory, and it is an active area of research. Physicists continue to explore the implications and predictions of string theory and are working towards finding ways to test its validity through experimental or observational evidence.\n",
      "\n",
      "In summary, physicists believe that string theory has the potential to provide a unified theory because it offers a framework that can potentially explain all fundamental particles and forces within a single consistent theory, while incorporating both quantum mechanics and general relativity.\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536df54-a985-401e-99e7-212004379618",
   "metadata": {},
   "source": [
    "## Task 2: Model Augmentation by Prompts\n",
    "Our chat model only has the knowledge based on the data used to train it. If we ask questions outside of the training data, then we won't get answers from the model. In the following example, let's ask the model what is LangChain Expression Language (LCEL) in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e182b4-3b48-4d28-88dd-27abc9ad4a1c",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 5005,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1703639560323,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages.append(res)\nprompt = HumanMessage(content=\"Can you tell me about the LangChain Expression Language (LCEL) in LangChain?\")\nmessages.append(prompt)\n\nres = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I'm not familiar with the specific details of the LangChain Expression Language (LCEL) in LangChain. It seems to be a domain-specific language or expression language specific to the LangChain platform. As an AI language model, my knowledge is based on general information and may not encompass the specifics of every specialized language or platform. \n",
      "\n",
      "If you require detailed information about the LangChain Expression Language, I would recommend referring to the official documentation or resources provided by LangChain or reaching out to their support or community for more specific information. They would be better equipped to assist you with the details of LCEL and how it is used within the LangChain platform.\n"
     ]
    }
   ],
   "source": [
    "messages.append(res)\n",
    "prompt = HumanMessage(content=\"Can you tell me about the LangChain Expression Language (LCEL) in LangChain?\")\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46c06c-d226-49ce-b467-78408945ff4f",
   "metadata": {},
   "source": [
    "one way of feeding knowledge into LLMs is called _source knowledge_. It refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. Let me copy the description of LCEL from the LangChain documentation, and feed it to the prompt to the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711778a5-76d5-40d2-8ed1-2b12a89a474d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1703639560373,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# A description of LLMChains, Chains, and LangChain \nllmchain_information = [\n    \"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\",\n    \"When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\",\n    \"For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server\"\n]\nlen(llmchain_information)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A description of LLMChains, Chains, and LangChain \n",
    "llmchain_information = [\n",
    "    \"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\",\n",
    "    \"When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\",\n",
    "    \"For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server\"\n",
    "]\n",
    "len(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63762f2-0cb2-48ee-8d51-8fc6b3165fcd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1703639560421,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "source_knowledge = \"\\n\".join(llmchain_information)\nsource_knowledge"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge = \"\\n\".join(llmchain_information)\n",
    "source_knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe8f0-4b6a-4227-8b40-0ab501c13ee5",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a71fbf-6b81-42f5-8073-ddfd3139e351",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1703639560468,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "query = \"Can you tell me about the LangChain Expression Language (LCEL) in LangChain?\"\n\naugmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\""
   },
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LangChain Expression Language (LCEL) in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n",
    "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e5faa0-e6dd-43a1-ba33-96fdeea31f9e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1703639560521,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(augmented_prompt)",
    "outputsMetadata": {
     "0": {
      "height": 337,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query. If some information is not provided within\n",
      "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
      "\n",
      "Contexts:\n",
      "LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\n",
      "When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\n",
      "For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server\n",
      "\n",
      "Query: Can you tell me about the LangChain Expression Language (LCEL) in LangChain?\n"
     ]
    }
   ],
   "source": [
    "print(augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e0ee47-69f2-4f21-b48d-279e9ae5df3e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1703639560568,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(messages[-1])",
    "outputsMetadata": {
     "0": {
      "height": 277,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Can you tell me about the LangChain Expression Language (LCEL) in LangChain?' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "print(messages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "901893e6-726d-4a42-805f-ee2d24b3b8a2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1703639560620,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "messages[-1] = HumanMessage(content=augmented_prompt)",
    "outputsMetadata": {
     "0": {
      "height": 368,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "messages[-1] = HumanMessage(content=augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebbf365d-0215-4f6f-bb5b-146323b559b4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 4478,
    "lastExecutedAt": 1703639565098,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "res = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 337,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LangChain Expression Language (LCEL) is a declarative way to compose chains together in LangChain. It was designed to support putting prototypes into production without any code changes. LCEL allows you to build chains with the best possible time-to-first-token, meaning you can receive incremental chunks of output at the same rate as the raw tokens are generated by the Language Model (LLM) provider. It also enables accessing the results of intermediate steps before the final output is produced, which can be useful for informing end-users about ongoing processes or for debugging purposes. Additionally, LCEL supports streaming intermediate results and is available on every LangServe server.\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae1f20-1e41-4e4f-a08e-09d0167170ba",
   "metadata": {},
   "source": [
    "It is clear that with external knowledge (source knowledge), the answer was much more relavant to our question. However, this method only applies if we only need to feed small volume of knowlege, and we know the knowlege we feed is relavant to our questions. What if we have large volumes of knowlege to augment the model, and we need the knowledge relavant to our questions to be retrived and used by the model according to our queries? We will use vector database to achieve this  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa3cfe-9223-4593-85fd-6c21f788d46e",
   "metadata": {},
   "source": [
    "## Task 3: Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4cd0f-4d29-4667-bf90-2603bae73759",
   "metadata": {},
   "source": [
    "In this task, pinecone will be used to store and retrieve the information about llama-2 model. The dataset contains paper published on ArXiv, which will serve as the external knowledge base for our chatbot. The dataset is from the Hugging Face Datasets library and [the `\"jamescalam/llama-2-arxiv-papers\"` dataset](https://huggingface.co/datasets/jamescalam/llama-2-arxiv-papers-chunked). Each entry in the dataset represents a \"chunk\" of text from these papers. \n",
    "The RAG strategy shown here is useful especially in scientific applications, since augment of knowledge base using publications is a common practice in scientific discovery and research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f97002-b358-4a33-bb50-0401feda259b",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 601,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1703639565700,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# import the dataset and load the dataset\nfrom datasets import load_dataset\n\ndata = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\ndata",
    "outputsMetadata": {
     "1": {
      "height": 77,
      "type": "stream"
     },
     "6": {
      "height": 77,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset and load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/llama-2-arxiv-papers-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0beaa-68f8-4cc3-831f-1118e79f45b3",
   "metadata": {},
   "source": [
    "## Task 4: Building the Pipecone Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92558679-3d66-419d-b30d-783456b992e5",
   "metadata": {},
   "source": [
    "We now have a dataset that can serve as our chatbot knowledge base. In this task, the text content of each chunk is transformed to vectors by the text-embedding-ada-002 embedding model provided from OpenAI, and insert into the knowledge base hosted by Pinecone vector database for our chatbot to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e123a-23f5-4fbb-a8dd-226dbdbd5b7f",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "The process to set up the knowledge base is described by the following steps:\n",
    "- Initialize your connection to the Pinecone vector DB.\n",
    "- Create an index using the dimensionality of `text-embedding-ada-002` embedding model, which is 1536.\n",
    "- Initialize OpenAI's `text-embedding-ada-002` model with LangChain.\n",
    "- Populate the index with records (in this case from the Llama 2 dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be5dcfe-1fd1-432c-9c95-77572f27e2fe",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 668,
    "lastExecutedAt": 1703639566368,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\nimport pinecone\nimport time\n\n# connect to pinecone vector database using api key and environment key\npinecone.init(\n    api_key=os.environ[\"PINECONE_API_KEY\"],\n    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n)\n\n# define an unique index for the database\nindex_name = \"llama-2-rag\"\n\n# create the database and continuously check the status until the database spins\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name, dimension=1536, metric=\"cosine\"\n    )\n    while not pinecone.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)\n        \n# get the index object from the created database and check its status\nindex = pinecone.Index(index_name)\n\nindex.describe_index_stats()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import time\n",
    "\n",
    "# connect to pinecone vector database using api key and environment key\n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
    ")\n",
    "\n",
    "# define an unique index for the database\n",
    "index_name = \"llama-2-rag\"\n",
    "\n",
    "# create the database and continuously check the status until the database spins\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name, dimension=1536, metric=\"cosine\"\n",
    "    )\n",
    "    while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "        \n",
    "# get the index object from the created database and check its status\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79733e18-cb8a-4b0a-9335-d0bdc90529ff",
   "metadata": {},
   "source": [
    "The vector database has been established. Now we will load the data to the database.\n",
    "* we first instantiate an object of the embedding model\n",
    "* we then convert the dataset to pandas dataframe. Thanks to the `to_pandas` method!\n",
    "* we then load the rows of dataframe to vector database using chunks of every 100 rows\n",
    "  + here index i was used to track the batch id, and for each batch, rows between i and `i+i_end` were processed and inserted\n",
    "  + basically, each text chunk with a unique id, embeding vector, and metadata dictionary was inserted\n",
    "* the \"text\" field in the metadata will be used to retrieve the original text content for vetors similar to the query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc53547-8e89-4700-9a69-95bca1bedd30",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1703639566416,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.embeddings.openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae4bf4ac-16ff-4c70-baef-dcd2f746a200",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 72025,
    "lastExecutedAt": 1703639638442,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from tqdm import tqdm\n\ndata = data.to_pandas()\n\nbatch_size = 100\n\nfor i in tqdm(range(0, len(data), batch_size)):\n    i_end = min(i+batch_size, len(data))\n    batch = data.iloc[i:i_end]\n    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n    embeds = embed_model.embed_documents(texts)\n    metadata = [\n        {\"text\": x[\"chunk\"],\n         \"title\": x[\"title\"],\n         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n    ]\n    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n    index.upsert(vectors=zip(ids, embeds, metadata))",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:11<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = data.to_pandas()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(i+batch_size, len(data))\n",
    "    batch = data.iloc[i:i_end]\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for _, x in batch.iterrows()]\n",
    "    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    metadata = [\n",
    "        {\"text\": x[\"chunk\"],\n",
    "         \"title\": x[\"title\"],\n",
    "         \"source\": x[\"source\"]} for _, x in  batch.iterrows()\n",
    "    ]\n",
    "    # [(id1, embed1, metadata1), (id2, embed2, metadata2), ...]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db36f2-1ba2-4fdb-9e72-aabe8fc16882",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48aa5c57-c923-49fc-b1dd-d60f6fa26d64",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 135,
    "lastExecutedAt": 1703639638577,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "index.describe_index_stats()"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90e6fc-f627-40a2-8276-7df46b5d7155",
   "metadata": {},
   "source": [
    "## Task 5: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946741c8-6ad8-4aa4-afe4-e3357929918b",
   "metadata": {},
   "source": [
    "After building the knowledge base, we will connect it to our chatbot. In this step, we will use the content retrieved from the vector database as the context that will be included in our prompt, as demonstrated in Task 2.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "* Create a LangChain `vectorstore` object using our `index` and `embed_model`.\n",
    "* Queery the database to retrieve the information about Llama 2 that are relavant to our query.\n",
    "* Augment the query prompt and include the information retrived as context to the augmented prompt. This was done by the `augment_prompt` function\n",
    "* Ask the chatbot Llama 2 questions using the prompts/queries with and without RAG, and compare the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d91af8d9-9e0b-4246-ba92-08769b7f6017",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1703639638624,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain.vectorstores import Pinecone\n\ntext_field = \"text\"\n\n# define a vectorstore from pinecone indexed database, with the index value\n# embedding model and the key of the metadata of the database to retrieve as\n# the query results\nvectorstore = Pinecone(\n    index, embed_model.embed_query, text_field\n)"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# define a vectorstore from pinecone indexed database, with the index value\n",
    "# embedding model and the key of the metadata of the database to retrieve as\n",
    "# the query results\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7231db5-b65e-431c-87fb-e4600b5901e1",
   "metadata": {},
   "source": [
    "Now, query the vector store and retrive information about llama 2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11343325-0736-4b08-b85b-1d4735a0402d",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1962,
    "lastExecutedAt": 1703639640587,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "query = \"What is so special about Llama 2?\"\n\nvectorstore.similarity_search(query, k=3)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5d7a5-9429-4d85-b8a5-f85aa9ae8654",
   "metadata": {},
   "source": [
    "These information are the most similar/relavant to our query. We can then augment our query by using the retrieved content as the contexts of the query, as we did in task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b86b04-9630-4fb8-a4f9-9f5b38300623",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1703639640636,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def augment_prompt(query: str):\n    results = vectorstore.similarity_search(query, k=3)\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\nthe contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n\nContexts:\n{source_knowledge}\n\nQuery: {query}\"\"\"\n    return augmented_prompt"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If some information is not provided within\n",
    "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4f1a7-5d0b-4e7f-ab38-fbada51ba1d8",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba045f5-88d7-4ce3-9ae6-e96cafa96e02",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 209,
    "lastExecutedAt": 1703639640846,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(augment_prompt(query))",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query. If some information is not provided within\n",
      "the contexts below, do not include, and if the query cannot be answered with the below information, say \"I don't know\".\n",
      "\n",
      "Contexts:\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
      "\n",
      "Query: What is so special about Llama 2?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c70d-a1c2-414b-b09b-afbc046503df",
   "metadata": {},
   "source": [
    "Now, let's pass the augmented prompt to the chat model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f70d2a0-d5af-448b-8f00-0aec873ac250",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5196,
    "lastExecutedAt": 1703639646042,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(content=augment_prompt(query))\n\nmessages.append(prompt)\nres = chat(messages)\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 277,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided contexts, it is mentioned that Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The models, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, have been optimized for dialogue use cases. The Llama 2 models outperform open-source chat models on most benchmarks tested and are considered as a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. However, further details about what specifically makes Llama 2 special are not provided in the given information.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(content=augment_prompt(query))\n",
    "\n",
    "messages.append(prompt)\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3547c02-31ea-4847-8042-7095d22a0fe9",
   "metadata": {},
   "source": [
    "Let's try the prompt _without_ RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00e797e1-5526-416b-be41-88925fa14960",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1539,
    "lastExecutedAt": 1703639647581,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(\n    content=\"What safety measures were used in the development of llama 2?\"\n)\n\nres = chat(messages + [prompt])\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 117,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have any information about the safety measures used in the development of Llama 2 based on the provided contexts.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"What safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea55fa4-c8a5-436e-8934-8d90a4002c57",
   "metadata": {},
   "source": [
    "Due to the information provided in the previous augmented prompt, the model learned some knowledge about llama 2 from the HumanMessage included in the conversational history stored in `messages`, and therefore, the chatbot was able to respond. However, it doesn't know anything about the safety measures as we have not provided it with that information via the RAG pipeline. Let's try again but with RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "412a5fb9-5bbb-46cd-8636-6bcb7f575605",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2878,
    "lastExecutedAt": 1703639650459,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = HumanMessage(\n    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n)\n\nres = chat(messages + [prompt])\nprint(res.content)",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, the safety measures used in the development of Llama 2 include safety-specific data annotation and tuning, conducting red-teaming, employing iterative evaluations, and contributing to improving LLM safety. These measures were taken to increase the safety of the models and ensure responsible development. Unfortunately, the specific details of these safety measures are not mentioned in the given contexts.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\"What safety measures were used in the development of llama 2?\")\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d11dd-338b-4732-8315-7434a4e82d48",
   "metadata": {},
   "source": [
    "We get a better informed response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea5aba-3a90-4655-9efa-425bc880e0cc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, Pinecone was used as the vector database to retrieve external information relavant to the question. The retrieved information was included in the prompt as the contexts. Comparision of prompts with and without the augments clearly showed that augmented prompts provide more relavant answers, with the help of extra information provided by the contexts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
